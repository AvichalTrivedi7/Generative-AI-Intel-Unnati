{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvichalTrivedi7/Generative-AI-Intel-Unnati/blob/main/API_integration_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b9d490-f0aa-413e-b6bb-c9b10fe9b295",
      "metadata": {
        "id": "79b9d490-f0aa-413e-b6bb-c9b10fe9b295",
        "outputId": "9373436e-fa16-4e52-ffa9-5df413ec9fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: google-generativeai in ./.local/lib/python3.9/site-packages (0.8.6)\n",
            "Requirement already satisfied: textblob in ./.local/lib/python3.9/site-packages (0.19.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in ./.local/lib/python3.9/site-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in ./.local/lib/python3.9/site-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in ./.local/lib/python3.9/site-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in ./.local/lib/python3.9/site-packages (from google-generativeai) (2.47.0)\n",
            "Requirement already satisfied: protobuf in ./.local/lib/python3.9/site-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in ./.local/lib/python3.9/site-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.local/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.0)\n",
            "Requirement already satisfied: nltk>=3.9 in ./.local/lib/python3.9/site-packages (from textblob) (3.9.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./.local/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: click in ./.local/lib/python3.9/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from nltk>=3.9->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.9/site-packages (from nltk>=3.9->textblob) (2025.9.18)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in ./.local/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.local/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.local/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.9/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in ./.local/lib/python3.9/site-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in ./.local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in ./.local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d7b5b2-d90e-4424-a722-8b8420860243",
      "metadata": {
        "id": "91d7b5b2-d90e-4424-a722-8b8420860243"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai textblob\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCDj1tzowiRjncbJYrcQeRl73MwWjTsZ_E\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1e21ca-a251-4140-b040-3b355233c6b3",
      "metadata": {
        "id": "ff1e21ca-a251-4140-b040-3b355233c6b3",
        "outputId": "d4fc67fd-e08c-4d20-83a2-2aa87147bfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gargi.singh/.local/lib/python3.9/site-packages/google/api_core/_python_version_support.py:237: FutureWarning: You are using a non-supported Python version (3.9.18). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/gargi.singh/.local/lib/python3.9/site-packages/google/auth/__init__.py:54: FutureWarning: \n",
            "    You are using a Python version 3.9 past its end of life. Google will update\n",
            "    google-auth with critical bug fixes on a best-effort basis, but not\n",
            "    with any other fixes or features. Please upgrade your Python version,\n",
            "    and then update google-auth.\n",
            "    \n",
            "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
            "/home/gargi.singh/.local/lib/python3.9/site-packages/google/oauth2/__init__.py:40: FutureWarning: \n",
            "    You are using a Python version 3.9 past its end of life. Google will update\n",
            "    google-auth with critical bug fixes on a best-effort basis, but not\n",
            "    with any other fixes or features. Please upgrade your Python version,\n",
            "    and then update google-auth.\n",
            "    \n",
            "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
            "/home/gargi.singh/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/tmp/ipykernel_2063893/383450838.py:1: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  import google.generativeai as genai\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768204801.406942 2063893 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<generator object list_models at 0x7f549e486660>\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/embedding-gecko-001\n",
            "  Supported Methods: ['embedText', 'countTextTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash',\n",
            "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
            "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-pro',\n",
            "      base_model_id='',\n",
            "      version='2.5',\n",
            "      display_name='Gemini 2.5 Pro',\n",
            "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-pro\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-exp\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-001\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-exp-image-generation\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-lite-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite 001',\n",
            "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-lite-001\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite',\n",
            "      description='Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-lite\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-lite-preview-02-05\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model Name: models/gemini-2.0-flash-lite-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-exp-1206\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Preview TTS',\n",
            "      description='Gemini 2.5 Flash Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-preview-tts\n",
            "  Supported Methods: ['countTokens', 'generateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-pro-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Pro Preview TTS',\n",
            "      description='Gemini 2.5 Pro Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-pro-preview-tts\n",
            "  Supported Methods: ['countTokens', 'generateContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemma-3-1b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 1B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemma-3-1b-it\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemma-3-4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 4B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemma-3-4b-it\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemma-3-12b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 12B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemma-3-12b-it\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemma-3-27b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 27B',\n",
            "      description='',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemma-3-27b-it\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemma-3n-e4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E4B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemma-3n-e4b-it\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemma-3n-e2b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E2B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemma-3n-e2b-it\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Flash Latest',\n",
            "      display_name='Gemini Flash Latest',\n",
            "      description='Latest release of Gemini Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-flash-latest\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-flash-lite-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Flash-Lite Latest',\n",
            "      display_name='Gemini Flash-Lite Latest',\n",
            "      description='Latest release of Gemini Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-flash-lite-latest\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Pro Latest',\n",
            "      display_name='Gemini Pro Latest',\n",
            "      description='Latest release of Gemini Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-pro-latest\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash-Lite',\n",
            "      description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-lite\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-image-preview',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Nano Banana',\n",
            "      description='Gemini 2.5 Flash Preview Image',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-image-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-image',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Nano Banana',\n",
            "      description='Gemini 2.5 Flash Preview Image',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-image\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Flash Preview 09-2025',\n",
            "      display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
            "      description='Gemini 2.5 Flash Preview Sep 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-preview-09-2025\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-09-25',\n",
            "      display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
            "      description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-lite-preview-09-2025\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-3-pro-preview',\n",
            "      base_model_id='',\n",
            "      version='3-pro-preview-11-2025',\n",
            "      display_name='Gemini 3 Pro Preview',\n",
            "      description='Gemini 3 Pro Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-3-pro-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-3-flash-preview',\n",
            "      base_model_id='',\n",
            "      version='3-flash-preview-12-2025',\n",
            "      display_name='Gemini 3 Flash Preview',\n",
            "      description='Gemini 3 Flash Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-3-flash-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-3-pro-image-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Nano Banana Pro',\n",
            "      description='Gemini 3 Pro Image Preview',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-3-pro-image-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/nano-banana-pro-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Nano Banana Pro',\n",
            "      description='Gemini 3 Pro Image Preview',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/nano-banana-pro-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-robotics-er-1.5-preview',\n",
            "      base_model_id='',\n",
            "      version='1.5-preview',\n",
            "      display_name='Gemini Robotics-ER 1.5 Preview',\n",
            "      description='Gemini Robotics-ER 1.5 Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-robotics-er-1.5-preview\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      description='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-computer-use-preview-10-2025\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/deep-research-pro-preview-12-2025',\n",
            "      base_model_id='',\n",
            "      version='deepthink-exp-05-20',\n",
            "      display_name='Deep Research Pro Preview (Dec-12-2025)',\n",
            "      description='Preview release (December 12th, 2025) of Deep Research Pro',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/deep-research-pro-preview-12-2025\n",
            "  Supported Methods: ['generateContent', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/embedding-001\n",
            "  Supported Methods: ['embedContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/text-embedding-004\n",
            "  Supported Methods: ['embedContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-embedding-exp-03-07',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental 03-07',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/gemini-embedding-exp-03-07\n",
            "  Supported Methods: ['embedContent', 'countTextTokens', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-embedding-exp',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/gemini-embedding-exp\n",
            "  Supported Methods: ['embedContent', 'countTextTokens', 'countTokens']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/gemini-embedding-001\n",
            "  Supported Methods: ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n",
            "Model Name: models/aqa\n",
            "  Supported Methods: ['generateAnswer']\n",
            "--------------------------------------------------\n",
            "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
            "      base_model_id='',\n",
            "      version='01',\n",
            "      display_name='Imagen 4 (Preview)',\n",
            "      description='Vertex served Imagen 4.0 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/imagen-4.0-generate-preview-06-06\n",
            "  Supported Methods: ['predict']\n",
            "--------------------------------------------------\n",
            "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
            "      base_model_id='',\n",
            "      version='01',\n",
            "      display_name='Imagen 4 Ultra (Preview)',\n",
            "      description='Vertex served Imagen 4.0 ultra model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/imagen-4.0-ultra-generate-preview-06-06\n",
            "  Supported Methods: ['predict']\n",
            "--------------------------------------------------\n",
            "Model(name='models/imagen-4.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Imagen 4',\n",
            "      description='Vertex served Imagen 4.0 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/imagen-4.0-generate-001\n",
            "  Supported Methods: ['predict']\n",
            "--------------------------------------------------\n",
            "Model(name='models/imagen-4.0-ultra-generate-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Imagen 4 Ultra',\n",
            "      description='Vertex served Imagen 4.0 ultra model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/imagen-4.0-ultra-generate-001\n",
            "  Supported Methods: ['predict']\n",
            "--------------------------------------------------\n",
            "Model(name='models/imagen-4.0-fast-generate-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Imagen 4 Fast',\n",
            "      description='Vertex served Imagen 4.0 Fast model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/imagen-4.0-fast-generate-001\n",
            "  Supported Methods: ['predict']\n",
            "--------------------------------------------------\n",
            "Model(name='models/veo-2.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Veo 2',\n",
            "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
            "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
            "                   'https://console.cloud.google.com/billing to enable it.'),\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/veo-2.0-generate-001\n",
            "  Supported Methods: ['predictLongRunning']\n",
            "--------------------------------------------------\n",
            "Model(name='models/veo-3.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Veo 3',\n",
            "      description='Veo 3',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/veo-3.0-generate-001\n",
            "  Supported Methods: ['predictLongRunning']\n",
            "--------------------------------------------------\n",
            "Model(name='models/veo-3.0-fast-generate-001',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Veo 3 fast',\n",
            "      description='Veo 3 fast',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/veo-3.0-fast-generate-001\n",
            "  Supported Methods: ['predictLongRunning']\n",
            "--------------------------------------------------\n",
            "Model(name='models/veo-3.1-generate-preview',\n",
            "      base_model_id='',\n",
            "      version='3.1',\n",
            "      display_name='Veo 3.1',\n",
            "      description='Veo 3.1',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/veo-3.1-generate-preview\n",
            "  Supported Methods: ['predictLongRunning']\n",
            "--------------------------------------------------\n",
            "Model(name='models/veo-3.1-fast-generate-preview',\n",
            "      base_model_id='',\n",
            "      version='3.1',\n",
            "      display_name='Veo 3.1 fast',\n",
            "      description='Veo 3.1 fast',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model Name: models/veo-3.1-fast-generate-preview\n",
            "  Supported Methods: ['predictLongRunning']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Flash Native Audio Latest',\n",
            "      display_name='Gemini 2.5 Flash Native Audio Latest',\n",
            "      description='Latest release of Gemini 2.5 Flash Native Audio',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-native-audio-latest\n",
            "  Supported Methods: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
            "      description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-native-audio-preview-09-2025\n",
            "  Supported Methods: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------------------------------------\n",
            "Model(name='models/gemini-2.5-flash-native-audio-preview-12-2025',\n",
            "      base_model_id='',\n",
            "      version='12-2025',\n",
            "      display_name='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
            "      description='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model Name: models/gemini-2.5-flash-native-audio-preview-12-2025\n",
            "  Supported Methods: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "'''Model Listing: Verifies available models (e.g., 2.5 series with 1M+ token limits) for selection,\n",
        "aiding model choice for long-context NLP tasks like suicide text analysis.'''\n",
        "\n",
        "models = genai.list_models()\n",
        "print(models)\n",
        "for model in models:\n",
        "    print(model)\n",
        "    print(f\"Model Name: {model.name}\")\n",
        "    print(f\"  Supported Methods: {model.supported_generation_methods}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8cf73d-73c0-4a77-801c-6c48243d51f3",
      "metadata": {
        "id": "fb8cf73d-73c0-4a77-801c-6c48243d51f3",
        "outputId": "5c921b29-8ffe-4d88-9e6e-027d71092589"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1768204802.095409 2063893 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Processing (NLP) is not just a subfield of Artificial Intelligence; it's a **cornerstone** that enables AI to understand, interact with, and learn from the human world. Without NLP, AI's capabilities would be severely limited, confined largely to numerical data and structured environments.\n",
            "\n",
            "Here's a breakdown of its critical importance:\n",
            "\n",
            "1.  **Natural Human-Computer Interaction (HCI):**\n",
            "    *   **Bridging the Communication Gap:** Humans communicate primarily through language (spoken and written). For AI to become truly intelligent and useful, it must understand and generate language. NLP allows humans to interact with machines in the most natural way possible, rather than having to learn complex programming languages or commands.\n",
            "    *   **Ubiquitous Interfaces:** Think of virtual assistants like Siri, Alexa, and Google Assistant, or chatbots in customer service. These are prime examples of NLP enabling intuitive, conversational interfaces that make AI accessible to everyone.\n",
            "\n",
            "2.  **Accessing and Understanding Vast Amounts of Human Knowledge:**\n",
            "    *   **Unstructured Data:** The overwhelming majority of human knowledge, information, and communication exists in unstructured text and speech data (web pages, books, articles, emails, social media, voice recordings).\n",
            "    *   **Information Retrieval and Extraction:** NLP allows AI systems to \"read,\" comprehend, summarize, and extract relevant information from this massive trove of data. This is crucial for search engines, knowledge base creation, scientific discovery, and competitive intelligence.\n",
            "    *   **Contextual Understanding:** Beyond just keywords, NLP helps AI grasp the context, intent, and meaning behind words and sentences, enabling more intelligent and relevant responses.\n",
            "\n",
            "3.  **Enabling Intelligent Behavior and Decision Making:**\n",
            "    *   **Sentiment Analysis:** NLP powers the ability to detect the emotional tone and sentiment behind text, which is invaluable for brand monitoring, customer feedback analysis, and understanding public opinion.\n",
            "    *   **Intent Recognition:** For AI to perform tasks, it needs to understand what a user *wants* to do. NLP helps systems parse user requests to identify their underlying intent.\n",
            "    *   **Reasoning and Inference:** Advanced NLP models can draw inferences and reason based on the text they've processed, contributing to more sophisticated AI decision-making processes, for example, in legal document review or medical diagnostics.\n",
            "\n",
            "4.  **Facilitating Learning and Adaptation for AI Systems:**\n",
            "    *   **Data for Machine Learning:** NLP provides the tools to preprocess and featurize textual data, making it suitable for various machine learning algorithms. Large Language Models (LLMs) like GPT-3/4 are trained on colossal amounts of text data, enabling them to learn patterns, facts, and even \"reasoning\" from human language.\n",
            "    *   **Reinforcement Learning from Human Feedback (RLHF):** In cutting-edge AI, humans provide feedback in natural language to guide models to generate more desirable outputs, which requires robust NLP capabilities.\n",
            "\n",
            "5.  **Developing More Human-like AI:**\n",
            "    *   **Turing Test:** The ability to converse intelligently and indistinguishably from a human is a key benchmark for artificial intelligence (the Turing Test), directly relying on advanced NLP.\n",
            "    *   **Empathy and Nuance:** As AI evolves, NLP aims to enable systems to understand more subtle human nuances like sarcasm, humor, irony, and cultural context, making AI interactions richer and more empathetic.\n",
            "    *   **Content Generation:** Beyond understanding, NLP is crucial for AI to *generate* coherent, contextually relevant, and even creative text, from writing articles and code to composing poetry.\n",
            "\n",
            "6.  **Expanding the Scope of AI Applications:**\n",
            "    *   **Healthcare:** Analyzing patient notes, medical literature, and diagnostic reports.\n",
            "    *   **Finance:** Processing financial news, reports, and predicting market trends.\n",
            "    *   **Legal:** Document review, contract analysis, and legal research.\n",
            "    *   **Education:** Personalized learning, essay grading, and content creation.\n",
            "    *   **Accessibility:** Providing text-to-speech and speech-to-text for individuals with disabilities.\n",
            "\n",
            "In essence, **NLP is the bridge that connects the computational world of AI with the human world of language and communication.** Without it, AI would be a powerful but largely isolated intelligence, unable to fully tap into human knowledge, interact naturally, or achieve its full potential in solving real-world human problems. As AI becomes more integrated into our daily lives, the importance of robust and sophisticated NLP will only continue to grow.\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Explain the importance of Natural Language Processing\n",
        "in Artificial Intelligence.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "generated_text = response.text\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1f6cac-a58a-4297-ac62-1d6c17df7998",
      "metadata": {
        "id": "9a1f6cac-a58a-4297-ac62-1d6c17df7998"
      },
      "source": [
        " ## Install  Authenticate  Explore  Generate  Analyze creates end-to-end prototype matching your \"Gemini  NLP extraction\" requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b392428-3cd1-4847-96f2-d59f2c1801cc",
      "metadata": {
        "id": "0b392428-3cd1-4847-96f2-d59f2c1801cc",
        "outputId": "548782f8-606e-4da9-c241-a6b18209e0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('ai', 18), ('nlp', 16), ('human', 10), ('language', 7), ('data', 7), ('text', 5), ('learning', 5), ('natural', 4), ('intelligence', 4), ('understand', 4)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/gargi.singh/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gargi.singh/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "tokens = word_tokenize(generated_text.lower())\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "word_freq = Counter(filtered_tokens)\n",
        "print(word_freq.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6adf1a68-ab75-4079-bdb6-8748fe0e0f0a",
      "metadata": {
        "id": "6adf1a68-ab75-4079-bdb6-8748fe0e0f0a",
        "outputId": "4c3c5271-9e74-4a0c-b55a-b513d24d8cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Polarity: 0.15754972875226037\n",
            "Sentiment Subjectivity: 0.5276823387582882\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(generated_text)\n",
        "\n",
        "print(\"Sentiment Polarity:\", blob.sentiment.polarity)\n",
        "print(\"Sentiment Subjectivity:\", blob.sentiment.subjectivity)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}