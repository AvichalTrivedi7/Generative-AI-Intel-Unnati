{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvichalTrivedi7/Generative-AI-Intel-Unnati/blob/main/Recipe_preprocessing_Gen_AI_Practice_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e549026a",
      "metadata": {
        "id": "e549026a"
      },
      "source": [
        "\n",
        "# Recipe Preprocessing (NLP Pipeline)\n",
        "\n",
        "This notebook cleans a free-text recipe paragraph and extracts a structured list of ingredients using lightweight NLP preprocessing.\n",
        "\n",
        "**Pipeline:** cleaning → lowercasing → tokenization → stopword removal → lemmatization & stemming → ingredient extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c02218",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4c02218",
        "outputId": "76290144-356c-4f2e-8480-1f79be7d93ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Recipe Preprocessing Assignment\n",
        "\n",
        "# Download required NLTK resources\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Input Recipe Paragraph\n",
        "\n",
        "text = \"\"\"To make fried rice, take 2 cups of rice, 1 tablespoon oil,\n",
        "2 onions, 1 carrot, 1/2 cup peas, and salt to taste. Cook for 10 minutes.\"\"\"\n",
        "\n",
        "print(\"Original Text:\\n\", text, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV12bZve5mDz",
        "outputId": "880a0ae0-186d-4f20-b713-a08c4484dfe4"
      },
      "id": "vV12bZve5mDz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " To make fried rice, take 2 cups of rice, 1 tablespoon oil, \n",
            "2 onions, 1 carrot, 1/2 cup peas, and salt to taste. Cook for 10 minutes. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Cleaning (remove numbers, measurements, punctuation)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\d+/\\d+\", \" \", text)             # remove fractions like 1/2\n",
        "    text = re.sub(r\"\\d+\", \" \", text)                 # remove pure numbers\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)            # keep only letters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()         # remove extra spaces\n",
        "    return text\n",
        "\n",
        "cleaned = clean_text(text)\n",
        "print(\"Cleaned Text:\\n\", cleaned, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkTS7BwY5pT-",
        "outputId": "b30164cd-c33d-4f6f-d0e7-6e077c92fa57"
      },
      "id": "KkTS7BwY5pT-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            " to make fried rice take cups of rice tablespoon oil onions carrot cup peas and salt to taste cook for minutes \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Tokenization\n",
        "\n",
        "tokens = word_tokenize(cleaned)\n",
        "print(\"Tokenized Words:\\n\", tokens, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWaEdEGw5qfK",
        "outputId": "763c8336-989a-48e3-a7c5-2c158732b4e8"
      },
      "id": "zWaEdEGw5qfK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words:\n",
            " ['to', 'make', 'fried', 'rice', 'take', 'cups', 'of', 'rice', 'tablespoon', 'oil', 'onions', 'carrot', 'cup', 'peas', 'and', 'salt', 'to', 'taste', 'cook', 'for', 'minutes'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Stopword Removal\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "print(\"After Stopword Removal:\\n\", filtered_tokens, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFnQItAm5sRX",
        "outputId": "66717e48-1ce9-442b-ab51-f318ef54c6fe"
      },
      "id": "EFnQItAm5sRX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stopword Removal:\n",
            " ['make', 'fried', 'rice', 'take', 'cups', 'rice', 'tablespoon', 'oil', 'onions', 'carrot', 'cup', 'peas', 'salt', 'taste', 'cook', 'minutes'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Lemmatization & Stemming\n",
        "# Helper function for POS tagging\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "pos_tags = nltk.pos_tag(filtered_tokens, lang='eng')\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "                     for word, tag in pos_tags]\n",
        "print(\"After Lemmatization:\\n\", lemmatized_tokens, \"\\n\")\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"After Stemming:\\n\", stemmed_tokens, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS8qfMg45rWP",
        "outputId": "e7fa6b65-0ff5-4a25-e352-466f6471bdbb"
      },
      "id": "tS8qfMg45rWP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Lemmatization:\n",
            " ['make', 'fried', 'rice', 'take', 'cup', 'rice', 'tablespoon', 'oil', 'onion', 'carrot', 'cup', 'pea', 'salt', 'taste', 'cook', 'minute'] \n",
            "\n",
            "After Stemming:\n",
            " ['make', 'fri', 'rice', 'take', 'cup', 'rice', 'tablespoon', 'oil', 'onion', 'carrot', 'cup', 'pea', 'salt', 'tast', 'cook', 'minut'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Extract Ingredients\n",
        "# (Simple dictionary lookup approach)\n",
        "\n",
        "INGREDIENT_LEXICON = {\"rice\",\"oil\",\"onion\",\"carrot\",\"peas\",\"salt\",\"pepper\",\"garlic\"}\n",
        "\n",
        "ingredients = [w for w in lemmatized_tokens if w in INGREDIENT_LEXICON]\n",
        "ingredients = list(dict.fromkeys(ingredients))  # remove duplicates, keep order\n",
        "print(\"Final List of Extracted Ingredients:\\n\", ingredients, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs9__2VQ5tQy",
        "outputId": "f0d72ca7-927e-42e4-d2fc-7a1cd0f65eab"
      },
      "id": "Qs9__2VQ5tQy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final List of Extracted Ingredients:\n",
            " ['rice', 'oil', 'onion', 'carrot', 'salt'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedf39d0",
      "metadata": {
        "id": "eedf39d0"
      },
      "source": [
        "\n",
        "### Reflection (Why preprocessing matters)\n",
        "Preprocessing reduces noise and standardizes text, which makes it easier\n",
        "for machines to extract structured information. Steps like stopword removal\n",
        "focus on meaningful words, while lemmatization and stemming reduce variations\n",
        "(e.g., onions → onion). This structured representation allows reliable extraction\n",
        "of ingredients from free-form recipe paragraphs.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}